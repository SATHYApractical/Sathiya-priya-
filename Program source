# Import libraries
from transformers import AutoModelForCausalLM, AutoTokenizer
import gradio as gr
import torch

# Load a pre-trained conversational model (DialoGPT-medium)
# Use AutoModelForCausalLM and AutoTokenizer instead of the pipeline
model_name = "microsoft/DialoGPT-medium"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Define chatbot function
def chat_with_bot(user_input, history=[]):
    # Encode the new user input
    new_user_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')

    # Append the new user input tokens to the chat history
    if history:
        # Assuming history is a list of tuples, extract the bot's last response
        # and concatenate it with the new user input for the model's context.
        # This is a simplified approach; for better conversation flow, you'd manage
        # the full conversation history tokens.
        # For simplicity, let's just use the new input for generation here.
        # A proper history management for context is more complex.
        # Let's clear the history and start fresh for each turn for now as an example.
        chat_history_ids = new_user_input_ids
    else:
        chat_history_ids = new_user_input_ids

    # Generate a response
    # Add `pad_token_id` to handle generation when input is short
    bot_output_ids = model.generate(
        chat_history_ids,
        max_length=1000,
        pad_token_id=tokenizer.eos_token_id
    )

    # Decode the bot's response
    bot_output = tokenizer.decode(bot_output_ids[:, chat_history_ids.shape[-1]:][0], skip_special_tokens=True)

    # Append the interaction to history for display
    history.append((user_input, bot_output))

    return bot_output, history

# Set up Gradio interface for interaction
with gr.Blocks() as demo:
    gr.Markdown("# AI Chatbot for Customer Support")
    chatbot_state = gr.State([])
    with gr.Row():
        user_input = gr.Textbox(label="Type your message here", container=False) # container=False can improve layout
    # Gradio Chatbot component handles the history list structure directly
    chat_output = gr.Chatbot()
    send_btn = gr.Button("Send")

    def respond(message, history):
        # The history provided by gr.Chatbot is a list of lists or tuples,
        # e.g., [[user_msg1, bot_msg1], [user_msg2, bot_msg2], ...]
        # The chat_with_bot function currently expects a list of tuples in a specific format.
        # Let's adjust respond to work with the gr.Chatbot history format.
        # The chat_with_bot function will manage the appending to this history list.
        response, updated_history = chat_with_bot(message, history)
        return "", updated_history # Clear the user input and return the updated history

    # Pass the message and the current chat history to the respond function
    # The output updates the user input textbox (clearing it) and the chat_output component
    send_btn.click(respond, [user_input, chat_output], [user_input, chat_output])

# Launch interface
demo.launch()
